{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['BookCrossing', 'Epinions', 'LFM360K', 'ML1M', 'ML20M', 'Yelp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "vali_ratio = 0.1\n",
    "test_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now process BookCrossing -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5107/5107 [00:02<00:00, 1888.31it/s]\n",
      "/tmp/ipykernel_49675/3015188812.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save(f'../mod_data/{dataset}/sep_data/user_train_like.npy', np.array(user_train_like))\n",
      "/tmp/ipykernel_49675/3015188812.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save(f'../mod_data/{dataset}/sep_data/user_vali_like.npy', np.array(user_vali_like))\n",
      "/tmp/ipykernel_49675/3015188812.py:84: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save(f'../mod_data/{dataset}/sep_data/user_test_like.npy', np.array(user_test_like))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now process Epinions -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8521/8521 [00:05<00:00, 1482.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now process LFM360K -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52966/52966 [02:51<00:00, 308.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now process ML1M -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:06<00:00, 980.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now process ML20M -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55845/55845 [08:19<00:00, 111.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now process Yelp -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13991/13991 [00:15<00:00, 894.58it/s] \n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print('now process', dataset, \"-----------------------------\")\n",
    "\n",
    "    df = pd.read_csv(f'../mod_data/{dataset}/{dataset}.csv', sep=',', names=['userId', 'itemId'] , skiprows=1)\n",
    "    user_list = df['userId'].unique()\n",
    "    item_list = df['itemId'].unique()\n",
    "\n",
    "    # Set size of validation set and test set\n",
    "    vali_size = int(vali_ratio * len(df))\n",
    "    test_size = int(test_ratio * len(df))\n",
    "\n",
    "    vali_idx = np.random.choice(np.arange(len(df)), vali_size, replace=False).tolist()\n",
    "    vali_df = df.loc[vali_idx]\n",
    "    df.drop(vali_idx, axis=0, inplace=True)\n",
    "    vali_df.reset_index(drop=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    test_idx = np.random.choice(np.arange(len(df)), test_size, replace=False).tolist()\n",
    "    test_df = df.loc[test_idx]\n",
    "    df.drop(test_idx, axis=0, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_array = df[['userId', 'itemId']].values\n",
    "    vali_array = vali_df[['userId', 'itemId']].values\n",
    "    test_array = test_df[['userId', 'itemId']].values\n",
    "\n",
    "    user_train_like = []\n",
    "    user_test_like = [] \n",
    "    user_vali_like = [] \n",
    "\n",
    "    # make sure that the test/validation set is not empty\n",
    "    for u in tqdm(user_list):\n",
    "        train_like = (train_array[list(np.where(train_array[:, 0] == u)[0]), 1]).astype(int)\n",
    "        vali_like = (vali_array[list(np.where(vali_array[:, 0] == u)[0]), 1]).astype(int)\n",
    "        test_like = (test_array[list(np.where(test_array[:, 0] == u)[0]), 1]).astype(int)\n",
    "\n",
    "        if len(train_like) == 0:\n",
    "            train_array = np.append(train_array, vali_array[list(np.where(vali_array[:, 0] == u)[0])], axis = 0)\n",
    "            train_array = np.append(train_array, test_array[list(np.where(test_array[:, 0] == u)[0])], axis = 0)\n",
    "            vali_array = np.delete(vali_array, np.where(vali_array[:, 0] == u)[0], axis = 0)\n",
    "            test_array = np.delete(test_array, np.where(test_array[:, 0] == u)[0], axis = 0)\n",
    "            train_like = (train_array[list(np.where(train_array[:, 0] == u)[0]), 1]).astype(int)\n",
    "            vali_like = (vali_array[list(np.where(vali_array[:, 0] == u)[0]), 1]).astype(int)\n",
    "            test_like = (test_array[list(np.where(test_array[:, 0] == u)[0]), 1]).astype(int)\n",
    "\n",
    "        # when validation set is empty\n",
    "        if len(vali_like) == 0:\n",
    "            if len(train_like) > len(test_like):\n",
    "                new_vali_idx = np.random.choice(np.arange(len(train_like)), size=1)\n",
    "                # add the record 'new_vali' to validation set\n",
    "                new_vali = train_like[new_vali_idx]\n",
    "                vali_like = np.array(new_vali)\n",
    "                train_like = np.delete(train_like, new_vali_idx)\n",
    "                train_array = np.delete(train_array, np.where((train_array[:, 0] == u) & (train_array[:, 1] == new_vali))[0], axis=0)\n",
    "                vali_array = np.append(vali_array, [[u, new_vali[0]]], axis=0)\n",
    "            else:\n",
    "                new_vali_idx = np.random.choice(np.arange(len(test_like)), size=1)\n",
    "                new_vali = test_like[new_vali_idx]\n",
    "                vali_like = np.array(new_vali)\n",
    "                test_like = np.delete(test_like, new_vali_idx)\n",
    "                test_array = np.delete(test_array, np.where((test_array[:, 0] == u) & (test_array[:, 1] == new_vali))[0], axis=0)\n",
    "                vali_array = np.append(vali_array, [[u, new_vali[0]]], axis=0)        \n",
    "\n",
    "        # when test set is empty\n",
    "        if len(test_like) == 0:\n",
    "            new_test_idx = np.random.choice(np.arange(len(train_like)), size=1)\n",
    "            # add the record 'new_test' to validation set\n",
    "            new_test = train_like[new_test_idx]\n",
    "            test_like = np.array(new_test)\n",
    "            train_like = np.delete(train_like, new_test_idx)\n",
    "            train_array = np.delete(train_array, np.where((train_array[:, 0] == u) & (train_array[:, 1] == new_test))[0], axis=0)\n",
    "            test_array = np.append(test_array, [[u, new_test[0]]], axis=0)\n",
    "\n",
    "        user_train_like.append(train_like)\n",
    "        user_vali_like.append(vali_like)\n",
    "        user_test_like.append(test_like)\n",
    "\n",
    "    np.save(f'../mod_data/{dataset}/sep_data/user_train_like.npy', np.array(user_train_like))\n",
    "    np.save(f'../mod_data/{dataset}/sep_data/user_vali_like.npy', np.array(user_vali_like))\n",
    "    np.save(f'../mod_data/{dataset}/sep_data/user_test_like.npy', np.array(user_test_like))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    train_df = pd.DataFrame({'userId': train_array[:, 0], 'itemId': train_array[:, 1]})\n",
    "    vali_df = pd.DataFrame({'userId': vali_array[:, 0], 'itemId': vali_array[:, 1]})\n",
    "    test_df = pd.DataFrame({'userId': test_array[:, 0], 'itemId': test_array[:, 1]})\n",
    "\n",
    "    # Save as .csv file\n",
    "    train_df.to_csv(f'../mod_data/{dataset}/sep_data/train_df.csv', index=False)\n",
    "    vali_df.to_csv(f'../mod_data/{dataset}/sep_data/vali_df.csv', index=False)\n",
    "    test_df.to_csv(f'../mod_data/{dataset}/sep_data/test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "907aeb4e8b69d3749718ea2f018ea403fed4114cf6a38aa67c8afab41dba42e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
