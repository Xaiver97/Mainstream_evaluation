{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ML1M'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show statistics of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zky/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset statistics: \n",
      "> No. of users: 6040\n",
      "> No. of items: 3706\n",
      "> No. of interactions: 1000209\n"
     ]
    }
   ],
   "source": [
    "rating_df = pd.read_csv('./ratings.dat', sep='::', names=[\"userId\", \"itemId\", \"rating\", \"timestamp\"])\n",
    "rating_df.drop(columns=['timestamp'], inplace=True)\n",
    "# Print the number of users, items and interactions\n",
    "print(\"Dataset statistics: \")\n",
    "print(f\"> No. of users: {len(rating_df['userId'].unique())}\")\n",
    "print(f\"> No. of items: {len(rating_df['itemId'].unique())}\")\n",
    "print(f\"> No. of interactions: {rating_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column of 'rating' and duplicate records\n",
    "rating_df.drop('rating', axis=1, inplace=True)\n",
    "rating_df.drop_duplicates(subset =['userId', 'itemId'], keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         userId  itemId  user_freq  item_freq\n",
      "0             1    1193         53       1725\n",
      "1             1     661         53        525\n",
      "2             1     914         53        636\n",
      "3             1    3408         53       1315\n",
      "4             1    2355         53       1703\n",
      "...         ...     ...        ...        ...\n",
      "1000204    6040    1091        341        373\n",
      "1000205    6040    1094        341       1229\n",
      "1000206    6040     562        341        478\n",
      "1000207    6040    1096        341        344\n",
      "1000208    6040    1097        341       2269\n",
      "\n",
      "[1000209 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Copy rating_df to rdf\n",
    "rdf = copy.copy(rating_df)\n",
    "# Calculate the total number of interactions of every user and item\n",
    "rdf['user_freq'] = rdf.groupby('userId')['userId'].transform('count')\n",
    "rdf['item_freq'] = rdf.groupby('itemId')['itemId'].transform('count')\n",
    "print(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds for user and item\n",
    "user_threshold = 0\n",
    "item_threshold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove users and items where their interactions less than threshold\n",
    "while (rdf['user_freq'].min() < user_threshold or rdf['item_freq'].min() < item_threshold) :\n",
    "    rdf.drop(rdf.index[rdf['user_freq'] < user_threshold], inplace=True)\n",
    "    rdf['item_freq'] = rdf.groupby('itemId')['itemId'].transform('count')\n",
    "    rdf.drop(rdf.index[rdf['item_freq'] < item_threshold], inplace=True)\n",
    "    rdf['user_freq'] = rdf.groupby('userId')['userId'].transform('count')\n",
    "    rdf['item_freq'] = rdf.groupby('itemId')['itemId'].transform('count')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total user:  6040\n",
      "total item:  3706\n",
      "sparsity: 0.044683625622312845\n"
     ]
    }
   ],
   "source": [
    "# Show the number of users, items and the sparsity after preprocessing\n",
    "usercnt = len(rdf['userId'].unique())\n",
    "itemcnt = len(rdf['itemId'].unique())\n",
    "print(\"total user: \", usercnt)\n",
    "print(\"total item: \", itemcnt)\n",
    "print('sparsity: ' + str(len(rdf) * 1.0 / (usercnt * itemcnt)))\n",
    "# Drop the column of 'user_freq' and 'item_freq'\n",
    "rdf.drop('user_freq', axis=1, inplace=True)\n",
    "rdf.drop('item_freq', axis=1, inplace=True)\n",
    "rdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renumber users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000209/1000209 [00:23<00:00, 41889.30it/s]\n"
     ]
    }
   ],
   "source": [
    "user_dic = dict()\n",
    "item_dic = dict()\n",
    "\n",
    "user_idx = 0\n",
    "item_idx = 0\n",
    "\n",
    "for row in tqdm(rdf.iterrows(), total=rdf.shape[0]):\n",
    "  if row[1][0] not in user_dic.keys():\n",
    "    user_dic[row[1][0]] = user_idx\n",
    "    user_idx += 1\n",
    "  # add a new book id with an index\n",
    "  if row[1][1] not in item_dic.keys():\n",
    "    item_dic[row[1][1]] = item_idx\n",
    "    item_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000209/1000209 [00:24<00:00, 40149.07it/s]\n"
     ]
    }
   ],
   "source": [
    "header = ['userId', 'itemId']\n",
    "with open(f'../../mod_data/{dataset}/{dataset}.csv', 'w', encoding='utf-8') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow(header)\n",
    "    for row in tqdm(rdf.iterrows(), total=rdf.shape[0]):\n",
    "        try:\n",
    "            writer.writerow([user_dic[row[1][0]], item_dic[row[1][1]]])\n",
    "        except KeyError as e:\n",
    "            print(e, row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "907aeb4e8b69d3749718ea2f018ea403fed4114cf6a38aa67c8afab41dba42e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
