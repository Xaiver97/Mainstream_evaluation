{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ML20M'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show statistics of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset statistics: \n",
      "> No. of users: 138493\n",
      "> No. of items: 26744\n",
      "> No. of interactions: 20000263\n"
     ]
    }
   ],
   "source": [
    "rating_df = pd.read_csv('./ratings.csv', sep=',', names=[\"userId\", \"itemId\", \"rating\", \"timestamp\"], skiprows=1)\n",
    "rating_df.drop(columns=['timestamp'], inplace=True)\n",
    "# Print the number of users, items and interactions\n",
    "print(\"Dataset statistics: \")\n",
    "print(f\"> No. of users: {len(rating_df['userId'].unique())}\")\n",
    "print(f\"> No. of items: {len(rating_df['itemId'].unique())}\")\n",
    "print(f\"> No. of interactions: {rating_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000263/20000263 [05:39<00:00, 58958.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Drop ratings less than 4\n",
    "for i in tqdm(range(len(rating_df))):\n",
    "    if rating_df.at[i, 'rating'] >= 4:\n",
    "        rating_df.at[i, 'rating'] = 1\n",
    "    else: \n",
    "        rating_df.at[i, 'rating'] = 0\n",
    "rating_df.drop(rating_df.index[rating_df['rating'] == 0], axis=0, inplace=True)\n",
    "# Drop the column of 'rating' and duplicate records\n",
    "rating_df.drop('rating', axis=1, inplace=True)\n",
    "rating_df.drop_duplicates(subset =['userId', 'itemId'], keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          userId  itemId  user_freq  item_freq\n",
      "6              1     151         88       6305\n",
      "7              1     223         88      15538\n",
      "8              1     253         88      12782\n",
      "9              1     260         88      42612\n",
      "10             1     293         88      19079\n",
      "...          ...     ...        ...        ...\n",
      "20000256  138493   66762        301         41\n",
      "20000257  138493   68319        301       1070\n",
      "20000258  138493   68954        301       6621\n",
      "20000259  138493   69526        301        591\n",
      "20000261  138493   70286        301       5229\n",
      "\n",
      "[9995410 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Copy rating_df to rdf\n",
    "rdf = copy.copy(rating_df)\n",
    "# Calculate the total number of interactions of every user and item\n",
    "rdf['user_freq'] = rdf.groupby('userId')['userId'].transform('count')\n",
    "rdf['item_freq'] = rdf.groupby('itemId')['itemId'].transform('count')\n",
    "print(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds for user and item\n",
    "user_threshold = 50\n",
    "item_threshold = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove users and items where their interactions less than threshold\n",
    "while (rdf['user_freq'].min() < user_threshold or rdf['item_freq'].min() < item_threshold) :\n",
    "    rdf.drop(rdf.index[rdf['user_freq'] < user_threshold], inplace=True)\n",
    "    rdf['item_freq'] = rdf.groupby('itemId')['itemId'].transform('count')\n",
    "    rdf.drop(rdf.index[rdf['item_freq'] < item_threshold], inplace=True)\n",
    "    rdf['user_freq'] = rdf.groupby('userId')['userId'].transform('count')\n",
    "    rdf['item_freq'] = rdf.groupby('itemId')['itemId'].transform('count')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total user:  55845\n",
      "total item:  8783\n",
      "sparsity: 0.0163016511143061\n"
     ]
    }
   ],
   "source": [
    "# Show the number of users, items and the sparsity after preprocessing\n",
    "usercnt = len(rdf['userId'].unique())\n",
    "itemcnt = len(rdf['itemId'].unique())\n",
    "print(\"total user: \", usercnt)\n",
    "print(\"total item: \", itemcnt)\n",
    "print('sparsity: ' + str(len(rdf) * 1.0 / (usercnt * itemcnt)))\n",
    "# Drop the column of 'user_freq' and 'item_freq'\n",
    "rdf.drop('user_freq', axis=1, inplace=True)\n",
    "rdf.drop('item_freq', axis=1, inplace=True)\n",
    "rdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renumber users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7995742/7995742 [03:08<00:00, 42347.61it/s]\n"
     ]
    }
   ],
   "source": [
    "user_dic = dict()\n",
    "item_dic = dict()\n",
    "\n",
    "user_idx = 0\n",
    "item_idx = 0\n",
    "\n",
    "for row in tqdm(rdf.iterrows(), total=rdf.shape[0]):\n",
    "  if row[1][0] not in user_dic.keys():\n",
    "    user_dic[row[1][0]] = user_idx\n",
    "    user_idx += 1\n",
    "  # add a new book id with an index\n",
    "  if row[1][1] not in item_dic.keys():\n",
    "    item_dic[row[1][1]] = item_idx\n",
    "    item_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7995742/7995742 [03:17<00:00, 40457.84it/s]\n"
     ]
    }
   ],
   "source": [
    "header = ['userId', 'itemId']\n",
    "with open(f'../../mod_data/{dataset}/{dataset}.csv', 'w', encoding='utf-8') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow(header)\n",
    "    for row in tqdm(rdf.iterrows(), total=rdf.shape[0]):\n",
    "        try:\n",
    "            writer.writerow([user_dic[row[1][0]], item_dic[row[1][1]]])\n",
    "        except KeyError as e:\n",
    "            print(e, row[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "907aeb4e8b69d3749718ea2f018ea403fed4114cf6a38aa67c8afab41dba42e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
